---
title: 集成学习：基础与算法
category: Ensemble Learning
tags:
  - Reading Notes
---

$$
\begin{align*}
\newcommand{\Hilbert}[1]{\mathscr{#1}}
\newcommand{\dd}{\operatorname{d}}
\newcommand{\op}{\hat}
\newcommand{\id}{\mathbf{I}}
\newcommand{\Tr}[1]{\operatorname{Tr}\left\lbrace#1\right\rbrace}
\newcommand{\realset}{\mathbb{R}}
\newcommand{\intset}{\mathbb Z }
\newcommand{\comset }{\mathbb C }
\newcommand{\innerproduct}[1]{\left\langle #1 \right\rangle}
\renewcommand{\vec}{\mathbf}
\newcommand{\spl}[1]{\langle{#1}\rangle}
\newcommand{\inner}[2]{\left\langle{#1,#2}\right\rangle}
\newcommand{\form}{\tilde}
\newcommand{\abs}[1]{\left\vert{#1}\right\vert}
\newcommand{\bra}[1]{\left\langle{#1}\right\vert }
\newcommand{\ket}[1]{\left| {#1}\right\rangle}
\newcommand{\braket}[2]{\left\langle {#1} \; \middle|\;{#2} \right\rangle }
\newcommand{\mani}{\mathcal}
\newcommand{\field}{\mathscr}
\newcommand{\Tspace}[1]{T\! {#1}}
\newcommand{\D}[2]{\frac{\d {#1}}{\d {#2} }}
\newcommand{\Partial}[2]{\frac{\partial {#1} }{\partial {#2} }}
\newcommand{\op}{\hat}
\newcommand{\uvec}{\hat}
\newcommand{\defas}{: =}
\newcommand{\isdefas}{= :}
\newcommand{\Eqn}[1]{\text{(Eqn. }\ref{#1}\text{)}}
\newcommand{\dual}{\tilde}
\newcommand{\vard}{\mathfrak{d}}
\newcommand{\vare}{\mathfrak{e}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\ii}{\mathrm{i}}
\newcommand{\blue}{\color{blue}}
\newcommand{\red}{\color{red}}
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\newcommand{\set}[1]{\left\lbrace{#1}\right\rbrace}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand\myeq{\stackrel{\mbox{adiabatic}}{=}}
\newcommand{\avg}[1]{\left\langle {#1} \right\rangle}
\end{align*}
$$

#### Bagging

Bagging算法使用bootstrap得到不同的训练子集，使用不同的训练子集用于训练不同的基分类器。在分类任务上投票来聚合基分类器，在回归问题上取平均来聚合基分类器。  

Bootstrap带来的额外好处：直接使用包外样本(out-of-bag) 来评估基分类器好坏及对算法的泛化误差进行估算

*类似6:4分训练集与测试集*

样本$x$ 上的包外预测：


$$
H^{\text {oob }}(\boldsymbol{x})=\underset{y \in \mathcal{Y}}{\arg \max } \sum_{t=1}^{T} \mathbb{I}\left(h_{t}(\boldsymbol{x})=y\right) \cdot \mathbb{I}\left(\boldsymbol{x} \notin D_{t}\right)
$$


Bagging泛化误差的包外估计：


$$
\operatorname{err}^{\mathrm{oob}}=\frac{1}{|D|} \sum_{(\boldsymbol{x}, y) \in D} \mathbb{I}\left(H^{\mathrm{oob}}(\boldsymbol{x}) \neq y\right)
$$

###### 稳定学习器

由于Bagging使用bootstrap来获得子样本，若基**分类器对训练数据分布不敏感**，得到的分类器之间会很相似，结合之后对泛化能力的提升将有限。



这类对训练数据不敏感的学习器称为“稳定学习器”，k近邻是“*高稳定分类器*”，决策树桩倾向于是“*稳定学习器*”，决策树是“*不稳定学习器*”。Bagging对k近邻等高稳定分类器不起作用，Bagging需要配合不稳定学习器使用。学习器越不稳定，效果提升越大。使用Bagging时，无需为决策树剪枝。



###### 收敛性

Bagging的预测效果会随着集成规模，即基学习器数目的增大而最终收敛。         

“因为使用了bootstrap，样本之间会有一定重合，基学习器之间并非相互独立。”    





##### References

《集成学习：基础与算法》 周志华

